
# Chapter 8: Generalization

## Bias-Variance

---

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/a7d4974c-58d7-49bb-85d8-d934c221d7a3/image.png)

ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ bias?

- ëª¨ë¸ì´ ë°ì´í„°ì˜ ì°¸ëœ ë¶„í¬ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚œ ì˜ˆì¸¡ì„ í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì²™ë„
- ë†’ì€ biasëŠ” ëª¨ë¸ì´ ë‹¨ìˆœí™”ë˜ì–´ ì‹¤ì œ ë°ì´í„°ë¥¼ ì œëŒ€ë¡œ ì„¤ëª…í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ë¥¼ ì˜ë¯¸
- ë‚®ì€ biasëŠ” ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì˜ ì„¤ëª…í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸

biasì™€ varianceê°€ ë§¤ìš° ë†’ì€ì§€ë¥¼ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ì„œ biasë‚˜ varianceê°€ ë§¤ìš° ë†’ì€ì§€ í™•ì¸í•  í•„ìš”ê°€ ìˆìŒ

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/24767b04-952d-4842-80a6-9944c7f2e0cb/image.png)

ì§€ê¸ˆë¶€í„° bias-varianceë¥¼ ìˆ˜ì‹ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ì˜ˆì •ì´ë‹¤.

$$ J(\theta) = \frac{1}{n}\sum^n_{i=1}(y^{(i)}-h_\theta(x^{(i)}))^2 $$

ì§€ê¸ˆ ìœ„ì˜ ì‹ì€ training loss ì‹ì´ë‹¤.

- $h_\theta(x) \to$ hypothesis

ê·¸ë¦¬ê³  test distributionìœ¼ë¡œë¶€í„° ì¶”ì¶œí•œ sample $(x_\text{test},y_\text{test}) \sim D$ì— ëŒ€í•œ test LossëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ L(\theta) = E_{(x_\text{test},y_\text{test}) \sim D}[(y^{(i)}-h_\theta(x^{(i)})^2] $$

ì´ëŸ´ ë•Œ, generalization gapì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$ \text{Generalization gap}=L(\theta) - J(\theta) $$

ì§ê´€ì ìœ¼ë¡œ ì•Œì•„ì•¼ í•  ê²ƒì€â€¦

- $L(\theta)$ëŠ” ìš°ë¦¬ê°€ ì œì–´í•  ìˆ˜ ì—†ëŠ” term
- $J(\theta)$ëŠ” ìš°ë¦¬ê°€ ì œì–´í•  ìˆ˜ ìˆëŠ” term, ì¦‰ opimizationì´ ê°€ëŠ¥í•œ term

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/adc93894-2ef6-4993-8778-772d736f2dff/image.png)

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/575e9fcf-5467-4591-a2ab-1c0b137cfe4d/image.png)

<aside> ğŸ’¡

**ê·¸ëŸ¼ ì–´ë–¤ ê²½ìš°ì—ì„œ overfittingì´ ì¼ì–´ë‚˜ëŠ”ì§€, underfittingì´ ì¼ì–´ë‚˜ëŠ” ì§€ë¥¼ í™•ì¸í•´ì•¼í•œë‹¤.**

- Loss functionì´ MSEë¡œ ì •ì˜ëœ ê²½ìš°ì—ë§Œ Biasì™€ varianceì— ëŒ€í•œ ìˆ˜ì‹ì ì¸ í•´ì„ì´ ê°€ëŠ¥í•˜ë‹¤.
- Loss functionì„ frequentistì  ê´€ì ì—ì„œ bias-variance ê´€ê³„ë¥¼ í•´ì„í•˜ê³  ìˆìŒì„ ìœ ì˜í•´ì•¼ í•œë‹¤.

ë‹¤ìŒ í˜ì´ì§€ë¥¼ í†µí•´ì„œ ìˆ˜ì‹ ê³¼ì •ì„ í™•ì¸í•´ ë³¼ ìˆ˜ ìˆë‹¤.

[Derive Bias-Variance Trade-off](https://www.notion.so/Derive-Bias-Variance-Trade-off-126b3d57d6448119a747c981276a8c9f?pvs=21)

</aside>

í•œ ì¤„ ìš”ì•½í•˜ìë©´â€¦

$$ \text{MSE} = \text{bias}^2 + \text{variance} $$

ìœ¼ë¡œ trade-off ê´€ê³„ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/60cd7a44-78fc-4f06-b9ea-990e89fbdc6f/image.png)

## Regularization

---

$$ \min_\theta \frac{1}{2} \sum^m_{i=1} ||y^{(i)}-\theta^Tx||^2 + \lambda ||\theta||^2 $$

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/44f185a7-aa11-4c6a-b464-99960425fd1a/image.png)

ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ê²½ìš°ì—ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ

$$ \argmax \sum^n_{i=1} \log p(y^{(i)} \mid x^{(i)} ; \theta) - \lambda ||\theta||^2 $$

**Text classificationì˜ ê²½ìš°**

- example $m = 100$, Vocabulary $n=10000$ ì²˜ëŸ¼

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/8199abdd-e695-407d-a9d8-03aa5d5ada80/image.png)

í•´ë‹¹ exampleì€ ì°¨ì›ë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë¶€ì¡±í•¨

- ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ê²½ìš°ì—ëŠ” $n \leq m$ ì¸ ê²½ìš°ì— íŒŒë¼ë¯¸í„° ìµœì í™”ê°€ ì¼ì–´ë‚¨
- ê·¸ëŸ¬ë‚˜ ì •ê·œí™”ë¥¼ ë¶™ì¸ë‹¤ë©´ exampleì´ ë¶€ì¡±í•œ ìƒí™©ì—ì„œë„ ì–´ëŠì •ë„ì˜ ìµœì í™”ê°€ ì¼ì–´ë‚¨

### ì •ê·œí™”ì˜ ë˜ ë‹¤ë¥¸ ê´€ì  (Bayesian approach)

$S_i \sim \{(x^{(i)}, y^{(i)})\}^{n-1}_{i=1}$ ë¼ëŠ” ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì •í•˜ëŠ” ê´€ì ì€ ë‘ ê°€ì§€ê°€ ì¡´ì¬í•œë‹¤.

**Frequentist Approach**

$$ p(\theta \mid S) \quad \rightarrow \quad\text{MLE} $$

- ì•Œë ¤ì ¸ ìˆì§€ ì•Šì§€ë§Œ í™•ì •ì ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” $\theta$ë¥¼ ì°¾ëŠ” ê³¼ì •ì´ë‹¤.
- ì‹¤ì œ ì¡´ì¬í•˜ëŠ” $\theta$ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. ($\argmax_\theta p(s \mid \theta)$)

**Bayesian Approach**

MLEê°€ ë¡œì§€ìŠ¤í‹± íšŒê·€ì„ì„ ê°€ì •í•´ë³´ë©´ ë² ì´ì¦ˆ ì •ë¦¬ì— ì˜í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•˜ê²Œ ëœë‹¤.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/27a2989f-7c08-41c6-bc69-9f6e3742529a/image.png)

ì—¬ê¸°ì„œ $\theta \sim N(0, \tau^2I)$ì´ë¼ê³  í•  ë•Œ, $p(\theta)$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•˜ê²Œ ëœë‹¤.

$$ p(\theta) = \frac{1}{\sqrt{2\pi}(\tau^2I)^{0.5}} \exp(-\theta^T (\tau^2I)^{-1} \theta) $$

$p(\theta)$ëŠ” $\theta$ì— ëŒ€í•œ ì‚¬ì „ ë¶„í¬ì´ê³ , MLEì™€ ì‚¬ì „ ë¶„í¬ë¥¼ ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ í†µí•´ì„œ ê²°í•©í•˜ê²Œ ëœë‹¤ë©´ $p(\theta \mid S)$ëŠ” ì •ê·œí™” ê¸°ìˆ ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ê²Œ ëœë‹¤.

**PRMLì˜ ë‚´ìš©ì„ í†µí•´ì„œ ì™œ ì •ê·œí™”ë¥¼ ê¸°ìˆ í•œ ê²ƒê³¼ ë™ì¼í•œ ê²ƒì¸ì§€ë¥¼ ìˆ˜ì‹ì ìœ¼ë¡œ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ë‹¤.**

- ê²°ë¡ ì ìœ¼ë¡œ log likelihood í˜•íƒœë¡œ ë„ìš°ë©´ ê·¸ë ‡ê²Œ ë¨

$$ \begin{aligned} \theta_{\text{MAP}} &= \arg\max_{\theta} P(X|\theta)P(\theta) \\&= \arg\max_{\theta} \log P(X|\theta) + \log P(\theta) \\&= \arg\max_{\theta} \log \prod_i P(x_i|\theta) + \log P(\theta) \\&= \arg\max_{\theta} \sum_i \log P(x_i|\theta) + \log P(\theta) \end{aligned} $$

$\log p(\theta)$ë¥¼ ê¸°ìˆ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤

$$ \log p(\theta) = \log \frac{1}{\sqrt{2\pi}(\tau^2I)^{0.5}} - \theta^T(\tau^2I)^{-1}\theta $$

<aside> ğŸ’¡

**ìœ„ì˜ ì‹ì„ í†µí•´ì„œ ì•„~~ì£¼ ì¬ë¯¸ìˆëŠ” íŠ¹ì§•ì„ ë³¼ ìˆ˜ ìˆë‹¤.**

**ê·¸ëŸ¬ë‚˜ ì´ëŠ” í†µê³„ì ì¸ ì§ê´€ì´ê¸° ë•Œë¬¸ì— ì•„ë˜ í˜ì´ì§€ë¡œ ë”°ë¡œ ì„œìˆ í•˜ë„ë¡ í•˜ê² ë‹¤.**

[About Bayesian Approch](https://www.notion.so/About-Bayesian-Approch-126b3d57d6448116bff0fc0596e07089?pvs=21)

</aside>

ê·¸ëŸ¼ Bayesian Approachì˜ íŠ¹ì§•ì´ë€?

$$ \argmax_\theta p(\theta \mid S) = \argmax_\theta p(S \mid \theta) \times p(\theta) $$

- $\theta$ë¥¼ ë¶ˆí™•ì‹¤í•˜ë‹¤. (= í™•ë¥ ì ì´ë‹¤.) ë‹¤ë§Œ, ë°ì´í„°ë¥¼ ë³´ê¸° ì „ê¹Œì§€ ë¬¸ì œì— ê´€í•œ ì‚¬ì „ ì§€ì‹ì„ ë°˜ì˜í•  ìˆ˜ ìˆë‹¤.
- ì‚¬ì „ ì§€ì‹ê³¼ í•¨ê»˜ ë°ì´í„°ë¥¼ ê´€ì¸¡í•œ í›„ì— ê°€ì¥ í™•ë¥ ì´ ë†’ì€ $\theta$ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ ìš°ë¦¬ì˜ ëª©í‘œì´ë‹¤
- ì´ê²ƒì„ **MaximumÂ A posterior Probability(MAP)ì´ë¼ê³  í•¨**

## The double descent phenomenon

---

**ì‚¬ì‹¤ ìœ„ì— ì¡´ì¬í•˜ëŠ” ì§ê´€ì˜ ê²½ìš°ì—ëŠ” ìµœê·¼ ë¨¸ì‹ ëŸ¬ë‹ (ë”¥ëŸ¬ë‹)ì˜ ê²½í–¥ê³¼ëŠ” ë™ë–¨ì–´ì ¸ ìˆë‹¤.**

- ìˆ˜ ì‹­ì–µê°œì˜ íŒŒë¼ë¯¸í„°ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , ê´„ëª©í• ë§Œí•œ ì„±ëŠ¥ì„ ë‚´ì—ˆê¸° ë•Œë¬¸ì´ë‹¤.
- ì´ëŠ” ê¸°ì¡´ Bias & Variance Error modelë¡œì¨ ì˜³ì¹˜ ì•ŠëŠ” ì§ê´€ì„ ê°€ì§€ê³  ìˆë‹¤.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/0f4d35bb-15da-4a17-a3ba-a40b063152a3/image.png)

**linear modelì˜ ê²½ìš° parameterì™€ data pointì˜ ê°œìˆ˜ê°€ ë¹„ìŠ·í•  ê²½ìš°ì— test Errorê°€ pickë¥¼ ì°ëŠ”ë‹¤.**

**modern regimeëŠ” Parameterì˜ ê°œìˆ˜ê°€ data pointë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë§ì„ ê²½ìš°ì— ë°œìƒí•œë‹¤.**

- over-parameterization ë° ê°•ë ¥í•œ computingì„ ê¸°ë°˜ìœ¼ë¡œ ê¹¨ë‹¬ì€ ì§ê´€ì´ ë˜ê² ë‹¤.

### Implicit regularization effect

ì´ëŸ¬í•œ ì´ìœ ë¡œ ìš”ì¦˜ ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì³ì—ì„œëŠ” regularizationì— ëŒ€í•´ì„œ í¬ê²Œ ìƒê° ì•ˆí•œë‹¤ê³  í•œë‹¤.

- Over-parameterizationì˜ ê²½ìš°ì— implicit regularization effectê°€ ë°œìƒí•œë‹¤.

ì™œ ì´ëŸ¬ëŠ” ì§€ë¥¼ ì„¤ëª…í•˜ê¸° ìœ„í•œ ëŠì„ì—†ëŠ” ì—°êµ¬ê°€ ì´ë¤„ì§€ê³  ìˆë‹¤ê³  í•œë‹¤. ë‹¤ë§Œ ì§ê´€ì ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/edf67436-8339-4e47-b6b0-2072d7efa88b/image.png)

**training Loss ê´€ì **

- íŒŒë¼ë¯¸í„°ê°€ ë§ì„ ìˆ˜ë¡, global minimaê°€ ì—¬ëŸ¬ ê°œê°€ ì¡´ì¬í•œë‹¤.

**test Loss ê´€ì **

- test Loss ê´€ì ì—ì„œëŠ” global minimaëŠ” train minima ì¤‘ì— í•˜ë‚˜ë§Œ ì„ íƒí•˜ë©´ ëœë‹¤

**Implicit regularization effectì—ì„œëŠ” íŒŒë¼ë¯¸í„°ì˜ ì´ˆê¸° ìœ„ì¹˜ì— ë”°ë¼ì„œ test Loss ìƒì—ì„œ ì„ í˜¸ë˜ëŠ” global minimaê°€ ê²°ì •!**

ì´í•´í•˜ê¸° ì‰½ê²Œ **Linear model** ìƒì—ì„œ ê¸°í•˜ì ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆë„ë¡ í•˜ì.

$$ J(\theta) = \frac{1}{n}\sum^n_{i=1}(y^{(i)}-\theta^Tx^{(i)})^2 \quad \{(x^{(1)}, y^{(1)}), \dots ,(x^{(i)}, y^{(i)})\} $$

Over-parameteried ìƒí™©ì„ ê°€ì •í•˜ê¸° ìœ„í•´ì„œ, $n<<d$ ìœ¼ë¡œ ê°€ì •í•œë‹¤.

- $n$ ëŠ” equationì˜ ê°œìˆ˜ì´ê³ , $d$ ëŠ” íŒŒë¼ë¯¸í„°ì˜ ê°œìˆ˜ì´ë‹¤.
- ì´ ê²½ìš°, ìœ„ì—ì„œ ë§í•œ ê²ƒê³¼ ê°™ì´ global minimaê°€ ë§ì•„ì§„ë‹¤.

trainingì„ í†µí•´ $y^{(i)}=\theta^Tx^{(i)}$ ë¥¼ ë§Œì¡±í•˜ëŠ” $\theta$ë¥¼ ì°¾ì•˜ë‹¤ê³  í•˜ì.

- ì´ë•Œ, ì„ í˜•ëŒ€ìˆ˜í•™ì—ì„œ rangeì™€ variable ê·¸ë¦¬ê³  null spaceì˜ ê´€ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ \text{rank}(X) + \text{nullity}(X) = d $$

- ì´ë•Œ, $0=X\theta$ì„ ë§Œì¡±í•˜ëŠ” null spaceì˜ ì°¨ì›ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$ \text{nullity}(X) = d - \text{rank}(X) = d-n $$

**ì´ì œ í•˜ë‚˜ì˜ equationê³¼ 3ê°œì˜ variableë¡œ ê°€ì •í•˜ì.**

- í•´ ê³µê°„ì€ 1ì°¨ì›ì´ ë˜ê³ , ì˜ ê³µê°„ì€ 2ì°¨ì›ì„ì„ ëª…ì‹¬í•˜ì. (ëª¨ë¥´ë©´ ì„ í˜•ëŒ€ìˆ˜í•™ ë³µìŠµ!)

$$ \text{Claim: Gradient Descent with Initial value } \theta=0 \\ \argmin ||\theta||^2_2 \quad \text{s.t. } J(\theta)=0 $$

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/b1908449-8021-4af3-a0dc-bde71d0eeca2/image.png)

ë¶€ë¶„ ê³µê°„ì—ì„œ ì˜ ê³µê°„ì— ê°€ì¥ ê°€ê¹Œìš´ ìµœì†Œ normì„ ê°–ëŠ” ìµœì ì˜ í•´ë¥¼ ì°¾ë„ë¡ í•œë‹¤.

- SGDê°€ $J(\theta)=0$ ì´ ë˜ëŠ” ê°€ì¥ ê°€ê¹Œìš´ ì§€ì ì„ ì°¾ì•„ê°€ê¸° ë•Œë¬¸ì— ì˜³ì€ ê°€ì •ì´ ëœë‹¤.
- $J(\theta)=0$ ì„ ë§Œì¡±í•˜ëŠ” $\theta$ë¥¼ spaní•˜ë„ë¡ í•œë‹¤

$$ \theta \in \text{span}\{\theta^{(1)},\dots,\theta^{(i)}\} $$