
# Chapter 8: Generalization

## Bias-Variance

---

![](https://i.imgur.com/dclVO8m.png)

ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ bias?

- ëª¨ë¸ì´ ë°ì´í„°ì˜ ì°¸ëœ ë¶„í¬ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚œ ì˜ˆì¸¡ì„ í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì²™ë„
- ë†’ì€ biasëŠ” ëª¨ë¸ì´ ë‹¨ìˆœí™”ë˜ì–´ ì‹¤ì œ ë°ì´í„°ë¥¼ ì œëŒ€ë¡œ ì„¤ëª…í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ë¥¼ ì˜ë¯¸
- ë‚®ì€ biasëŠ” ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì˜ ì„¤ëª…í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸

biasì™€ varianceê°€ ë§¤ìš° ë†’ì€ì§€ë¥¼ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ì„œ biasë‚˜ varianceê°€ ë§¤ìš° ë†’ì€ì§€ í™•ì¸í•  í•„ìš”ê°€ ìˆìŒ

![](https://i.imgur.com/jAVbTkv.png)

ì§€ê¸ˆë¶€í„° bias-varianceë¥¼ ìˆ˜ì‹ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ì˜ˆì •ì´ë‹¤.

$$ J(\theta) = \frac{1}{n}\sum^n_{i=1}(y^{(i)}-h_\theta(x^{(i)}))^2 $$

ì§€ê¸ˆ ìœ„ì˜ ì‹ì€ training loss ì‹ì´ë‹¤.

- $h_\theta(x) \to$ hypothesis

ê·¸ë¦¬ê³  test distributionìœ¼ë¡œë¶€í„° ì¶”ì¶œí•œ sample $(x_\text{test},y_\text{test}) \sim D$ì— ëŒ€í•œ test LossëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ L(\theta) = E_{(x_\text{test},y_\text{test}) \sim D}[(y^{(i)}-h_\theta(x^{(i)})^2] $$

ì´ëŸ´ ë•Œ, generalization gapì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$ \text{Generalization gap}=L(\theta) - J(\theta) $$

ì§ê´€ì ìœ¼ë¡œ ì•Œì•„ì•¼ í•  ê²ƒì€â€¦

- $L(\theta)$ëŠ” ìš°ë¦¬ê°€ ì œì–´í•  ìˆ˜ ì—†ëŠ” term
- $J(\theta)$ëŠ” ìš°ë¦¬ê°€ ì œì–´í•  ìˆ˜ ìˆëŠ” term, ì¦‰ opimizationì´ ê°€ëŠ¥í•œ term

![](https://i.imgur.com/5EGRHhj.png)

![](https://i.imgur.com/W5RimPl.png)

ğŸ’¡**ê·¸ëŸ¼ ì–´ë–¤ ê²½ìš°ì—ì„œ overfittingì´ ì¼ì–´ë‚˜ëŠ”ì§€, underfittingì´ ì¼ì–´ë‚˜ëŠ” ì§€ë¥¼ í™•ì¸í•´ì•¼í•œë‹¤.**

- Loss functionì´ MSEë¡œ ì •ì˜ëœ ê²½ìš°ì—ë§Œ Biasì™€ varianceì— ëŒ€í•œ ìˆ˜ì‹ì ì¸ í•´ì„ì´ ê°€ëŠ¥í•˜ë‹¤.
- Loss functionì„ frequentistì  ê´€ì ì—ì„œ bias-variance ê´€ê³„ë¥¼ í•´ì„í•˜ê³  ìˆìŒì„ ìœ ì˜í•´ì•¼ í•œë‹¤.

![](https://i.imgur.com/9utC0AE.png)

- frequentist ê´€ì ì—ì„œ ì°¸ì— í•´ë‹¹í•˜ëŠ” ê°€ì„¤ì´ ì¡´ì¬í•œë‹¤. ì´ë¥¼ $h^*(x)$ì´ë¼ê³  ì •ì˜í•˜ë„ë¡ í•œë‹¤.
- training dataset $S = \{x^{(i)}, y^{(i)}\}^n_{i=1}$ì´ ìˆë‹¤ê³  í•  ë•Œ ë‹¤ìŒê³¼ ê°™ì€ ê´€ê³„ì‹ì„ ê°–ëŠ”ë‹¤.

$$ y^{(i)}=h^*(x^{(i)})+\varepsilon $$

ì´ ë•Œ, $\varepsilon \sim N(0,\sigma^2)$ì„ ë”°ë¥´ë„ë¡ í•œë‹¤. (ìì—°ì ìœ¼ë¡œ ë°œìƒí•œ ë…¸ì´ì¦ˆì˜ ê²½ìš°ì—ëŠ” í•´ë‹¹ ë¶„í¬ë¥¼ ì „ì œë¡œ í•œë‹¤.)

dataset $S$ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì •í•œ ê°€ì„¤ì„ $\hat{h}_S$ì´ë¼ê³  ì •ì˜ë¥¼ í•œë‹¤.

- $y=h^*(x)+\varepsilon$ ê´€ê³„ì‹ìœ¼ë¡œ ì´ë£¨ì–´ì§„ test example $(x,y)$ì´ ìˆë‹¤ê³  í•˜ì
- ì´ë•Œ test exampleê³¼ ì¶”ì • ê°€ì„¤ ê°„ì— residual errorëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚´ë„ë¡ í•œë‹¤.

$$ \text{MSE}(x)=\mathbb{E}_{S,\varepsilon}[(y-h_S(x))^2] $$

MSEë¥¼ decomposeí•˜ì—¬ biasì™€ variance termì„ ë‚˜íƒ€ë‚´ë„ë¡ í•œë‹¤.

![](https://i.imgur.com/Jq7RKSK.png)

ìœ„ì˜ ì „ê°œì‹ì„ ì •ë¦¬í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ \text{MSE} = \sigma^2 + \mathbb{E}[(h^*(x)-h_S(x))^2] $$

---

**ë¬´í•œíˆ ë§ì€ ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ ì—¬ëŸ¬ ëª¨ë¸ì„ í›ˆë ¨í•œ í›„, ê·¸ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ê°’ì„ í‰ê· ë‚¸ í‰ê·  ëª¨ë¸ì„** **$h_{avg}(x) = \mathbb{E}_S[h_S(x)]$ìœ¼ë¡œ ì •ì˜**

- ì‹¤ì œë¡œëŠ” ë¬´í•œëŒ€ì˜ exampleì„ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í˜„ì‹¤ì—ì„œ êµ¬í˜„ ê°€ëŠ¥í•˜ì§€ ì•Šê³  ë¶„ì„ì ì¸ ëª©ì ìœ¼ë¡œ ì‚¬ìš©

**$h_{avg}$ëŠ” ë¬´í•œí•œ sampleì„ ê°€ì§„ ë‹¨ì¼ ë°ì´í„° ì„¸íŠ¸ì—ì„œ í›ˆë ¨í•˜ì—¬ ì–»ì€ ëª¨ë¸ê³¼ ê±°ì˜ ê°™ìŒ**

- **ë¬´í•œí•œ ìƒ˜í”Œì„ ê¸°ë°˜ìœ¼ë¡œ í›ˆë ¨í•œ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì€ ì œí•œëœ ë°ì´í„°ì…‹ì—ì„œ í›ˆë ¨ëœ ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì˜ í‰ê· ê³¼ ìœ ì‚¬**
- ì—¬ëŸ¬ ëª¨ë¸ì˜ í‰ê· ì„ ì·¨í•˜ëŠ” í‰ê·  ëª¨ë¸ì€ ê° ëª¨ë¸ë“¤ì´ ê°€ì§€ëŠ” Biasê°€ ìƒì‡„ë˜ì–´ ë” ë‚®ì€ Biasë¥¼ ê°€ì§€ëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ
- ì´ëŠ” Biasì— ëŒ€í•œ ì§ê´€ì  ì •ì˜ì™€ ì¼ì¹˜ â†’ ê°œë³„ ëª¨ë¸ë“¤ë³´ë‹¤ ë” ì¢‹ì€ Generalizationì„ ë³´ì—¬ì¤Œ
    - **[Note that]** bias: ëª¨ë¸ì´ ë°ì´í„°ì˜ ì°¸ëœ ë¶„í¬ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚œ ì˜ˆì¸¡ì„ í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì²™ë„

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/603409f3-9dbb-4201-8195-bf3516f05c48/image.png)

$$ \text{MSE}(x) = \sigma^2 + (h^*(x) - h_{\text{avg}}(x))^2 + \mathbb{E}[(h_{\text{avg}}(x) - h_S(x))^2] $$

**$\sigma^2$ â†’ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆ**

- ëª¨ë¸ì˜ ì„±ëŠ¥ì— ê´€ê³„ì—†ì´ ë°œìƒí•˜ëŠ” ì˜¤ì°¨

**$h^*(x) - h_{\text{avg}}(x))^2$ â†’ Biasì˜ ì œê³±**

- í‰ê·  ëª¨ë¸ì™€ ì°¸ëœ ëª¨ë¸ ì‚¬ì´ì˜ ì°¨ì´
- ëª¨ë¸ì´ ì‹¤ì œ ë°ì´í„° ë¶„í¬ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚˜ ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„
- ëª¨ë¸ì´ ê³¼í•˜ê²Œ ë‹¨ìˆœí™”ë˜ì–´ ë°ì´í„°ì˜ íŒ¨í„´ íŒŒì•…ì´ í˜ë“¤ìˆ˜ë¡ Biasê°€ ì»¤ì§ ($h_{avg}$ ê°ì†Œ)

**$\text{var}(h_S(x))$ â†’ ëª¨ë¸ì˜ Variance**

- ë°ì´í„°ì…‹ì— ì˜í•´ í›ˆë ¨ëœ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì´ í‰ê·  ëª¨ë¸ì™€ ì–¼ë§ˆë‚˜ ì¼ê´€ì„±ì´ ì—†ëŠ”ì§€?
- ì—¬ëŸ¬ ë°ì´í„°ì…‹ì— ë”°ë¥¸ ì˜ˆì¸¡ì˜ ë³€ë™ì„±ì„ ì˜ë¯¸
- Varianceê°€ í´ìˆ˜ë¡ ëª¨ë¸ì´ ë°ì´í„°ì— overfittingë˜ì–´, ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì˜ ì¼ë°˜í™”ë˜ì§€ ëª»í•¨

ì–´ì°¨í”¼ $\sigma$ëŠ” ë³€í•˜ì§€ ì•ŠëŠ” ìƒìˆ˜ì´ë¯€ë¡œ ì œì™¸í•œë‹¤ë©´â€¦

$$ \text{MSE} = \text{bias}^2 + \text{variance} $$

ê°€ ëœë‹¤.

í•œ ì¤„ ìš”ì•½í•˜ìë©´â€¦

$$ \text{MSE} = \text{bias}^2 + \text{variance} $$

ìœ¼ë¡œ trade-off ê´€ê³„ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/60cd7a44-78fc-4f06-b9ea-990e89fbdc6f/image.png)

## Regularization

---

$$ \min_\theta \frac{1}{2} \sum^m_{i=1} ||y^{(i)}-\theta^Tx||^2 + \lambda ||\theta||^2 $$

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/44f185a7-aa11-4c6a-b464-99960425fd1a/image.png)

ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ê²½ìš°ì—ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ

$$ \argmax \sum^n_{i=1} \log p(y^{(i)} \mid x^{(i)} ; \theta) - \lambda ||\theta||^2 $$

**Text classificationì˜ ê²½ìš°**

- example $m = 100$, Vocabulary $n=10000$ ì²˜ëŸ¼

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/8199abdd-e695-407d-a9d8-03aa5d5ada80/image.png)

í•´ë‹¹ exampleì€ ì°¨ì›ë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë¶€ì¡±í•¨

- ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ê²½ìš°ì—ëŠ” $n \leq m$ ì¸ ê²½ìš°ì— íŒŒë¼ë¯¸í„° ìµœì í™”ê°€ ì¼ì–´ë‚¨
- ê·¸ëŸ¬ë‚˜ ì •ê·œí™”ë¥¼ ë¶™ì¸ë‹¤ë©´ exampleì´ ë¶€ì¡±í•œ ìƒí™©ì—ì„œë„ ì–´ëŠì •ë„ì˜ ìµœì í™”ê°€ ì¼ì–´ë‚¨

### ì •ê·œí™”ì˜ ë˜ ë‹¤ë¥¸ ê´€ì  (Bayesian approach)

$S_i \sim \{(x^{(i)}, y^{(i)})\}^{n-1}_{i=1}$ ë¼ëŠ” ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì •í•˜ëŠ” ê´€ì ì€ ë‘ ê°€ì§€ê°€ ì¡´ì¬í•œë‹¤.

**Frequentist Approach**

$$ p(\theta \mid S) \quad \rightarrow \quad\text{MLE} $$

- ì•Œë ¤ì ¸ ìˆì§€ ì•Šì§€ë§Œ í™•ì •ì ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” $\theta$ë¥¼ ì°¾ëŠ” ê³¼ì •ì´ë‹¤.
- ì‹¤ì œ ì¡´ì¬í•˜ëŠ” $\theta$ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. ($\argmax_\theta p(s \mid \theta)$)

**Bayesian Approach**

MLEê°€ ë¡œì§€ìŠ¤í‹± íšŒê·€ì„ì„ ê°€ì •í•´ë³´ë©´ ë² ì´ì¦ˆ ì •ë¦¬ì— ì˜í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•˜ê²Œ ëœë‹¤.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/27a2989f-7c08-41c6-bc69-9f6e3742529a/image.png)

ì—¬ê¸°ì„œ $\theta \sim N(0, \tau^2I)$ì´ë¼ê³  í•  ë•Œ, $p(\theta)$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•˜ê²Œ ëœë‹¤.

$$ p(\theta) = \frac{1}{\sqrt{2\pi}(\tau^2I)^{0.5}} \exp(-\theta^T (\tau^2I)^{-1} \theta) $$

$p(\theta)$ëŠ” $\theta$ì— ëŒ€í•œ ì‚¬ì „ ë¶„í¬ì´ê³ , MLEì™€ ì‚¬ì „ ë¶„í¬ë¥¼ ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ í†µí•´ì„œ ê²°í•©í•˜ê²Œ ëœë‹¤ë©´ $p(\theta \mid S)$ëŠ” ì •ê·œí™” ê¸°ìˆ ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ê²Œ ëœë‹¤.

**PRMLì˜ ë‚´ìš©ì„ í†µí•´ì„œ ì™œ ì •ê·œí™”ë¥¼ ê¸°ìˆ í•œ ê²ƒê³¼ ë™ì¼í•œ ê²ƒì¸ì§€ë¥¼ ìˆ˜ì‹ì ìœ¼ë¡œ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ë‹¤.**

- ê²°ë¡ ì ìœ¼ë¡œ log likelihood í˜•íƒœë¡œ ë„ìš°ë©´ ê·¸ë ‡ê²Œ ë¨

$$ \begin{aligned} \theta_{\text{MAP}} &= \arg\max_{\theta} P(X|\theta)P(\theta) \\&= \arg\max_{\theta} \log P(X|\theta) + \log P(\theta) \\&= \arg\max_{\theta} \log \prod_i P(x_i|\theta) + \log P(\theta) \\&= \arg\max_{\theta} \sum_i \log P(x_i|\theta) + \log P(\theta) \end{aligned} $$

$\log p(\theta)$ë¥¼ ê¸°ìˆ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤

$$ \log p(\theta) = \log \frac{1}{\sqrt{2\pi}(\tau^2I)^{0.5}} - \theta^T(\tau^2I)^{-1}\theta $$

<aside> ğŸ’¡

**ìœ„ì˜ ì‹ì„ í†µí•´ì„œ ì•„~~ì£¼ ì¬ë¯¸ìˆëŠ” íŠ¹ì§•ì„ ë³¼ ìˆ˜ ìˆë‹¤.**

**ê·¸ëŸ¬ë‚˜ ì´ëŠ” í†µê³„ì ì¸ ì§ê´€ì´ê¸° ë•Œë¬¸ì— ì•„ë˜ í˜ì´ì§€ë¡œ ë”°ë¡œ ì„œìˆ í•˜ë„ë¡ í•˜ê² ë‹¤.**

[About Bayesian Approch](https://www.notion.so/About-Bayesian-Approch-126b3d57d6448116bff0fc0596e07089?pvs=21)

</aside>

ê·¸ëŸ¼ Bayesian Approachì˜ íŠ¹ì§•ì´ë€?

$$ \argmax_\theta p(\theta \mid S) = \argmax_\theta p(S \mid \theta) \times p(\theta) $$

- $\theta$ë¥¼ ë¶ˆí™•ì‹¤í•˜ë‹¤. (= í™•ë¥ ì ì´ë‹¤.) ë‹¤ë§Œ, ë°ì´í„°ë¥¼ ë³´ê¸° ì „ê¹Œì§€ ë¬¸ì œì— ê´€í•œ ì‚¬ì „ ì§€ì‹ì„ ë°˜ì˜í•  ìˆ˜ ìˆë‹¤.
- ì‚¬ì „ ì§€ì‹ê³¼ í•¨ê»˜ ë°ì´í„°ë¥¼ ê´€ì¸¡í•œ í›„ì— ê°€ì¥ í™•ë¥ ì´ ë†’ì€ $\theta$ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ ìš°ë¦¬ì˜ ëª©í‘œì´ë‹¤
- ì´ê²ƒì„ **MaximumÂ A posterior Probability(MAP)ì´ë¼ê³  í•¨**

## The double descent phenomenon

---

**ì‚¬ì‹¤ ìœ„ì— ì¡´ì¬í•˜ëŠ” ì§ê´€ì˜ ê²½ìš°ì—ëŠ” ìµœê·¼ ë¨¸ì‹ ëŸ¬ë‹ (ë”¥ëŸ¬ë‹)ì˜ ê²½í–¥ê³¼ëŠ” ë™ë–¨ì–´ì ¸ ìˆë‹¤.**

- ìˆ˜ ì‹­ì–µê°œì˜ íŒŒë¼ë¯¸í„°ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , ê´„ëª©í• ë§Œí•œ ì„±ëŠ¥ì„ ë‚´ì—ˆê¸° ë•Œë¬¸ì´ë‹¤.
- ì´ëŠ” ê¸°ì¡´ Bias & Variance Error modelë¡œì¨ ì˜³ì¹˜ ì•ŠëŠ” ì§ê´€ì„ ê°€ì§€ê³  ìˆë‹¤.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/0f4d35bb-15da-4a17-a3ba-a40b063152a3/image.png)

**linear modelì˜ ê²½ìš° parameterì™€ data pointì˜ ê°œìˆ˜ê°€ ë¹„ìŠ·í•  ê²½ìš°ì— test Errorê°€ pickë¥¼ ì°ëŠ”ë‹¤.**

**modern regimeëŠ” Parameterì˜ ê°œìˆ˜ê°€ data pointë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë§ì„ ê²½ìš°ì— ë°œìƒí•œë‹¤.**

- over-parameterization ë° ê°•ë ¥í•œ computingì„ ê¸°ë°˜ìœ¼ë¡œ ê¹¨ë‹¬ì€ ì§ê´€ì´ ë˜ê² ë‹¤.

### Implicit regularization effect

ì´ëŸ¬í•œ ì´ìœ ë¡œ ìš”ì¦˜ ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì³ì—ì„œëŠ” regularizationì— ëŒ€í•´ì„œ í¬ê²Œ ìƒê° ì•ˆí•œë‹¤ê³  í•œë‹¤.

- Over-parameterizationì˜ ê²½ìš°ì— implicit regularization effectê°€ ë°œìƒí•œë‹¤.

ì™œ ì´ëŸ¬ëŠ” ì§€ë¥¼ ì„¤ëª…í•˜ê¸° ìœ„í•œ ëŠì„ì—†ëŠ” ì—°êµ¬ê°€ ì´ë¤„ì§€ê³  ìˆë‹¤ê³  í•œë‹¤. ë‹¤ë§Œ ì§ê´€ì ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/edf67436-8339-4e47-b6b0-2072d7efa88b/image.png)

**training Loss ê´€ì **

- íŒŒë¼ë¯¸í„°ê°€ ë§ì„ ìˆ˜ë¡, global minimaê°€ ì—¬ëŸ¬ ê°œê°€ ì¡´ì¬í•œë‹¤.

**test Loss ê´€ì **

- test Loss ê´€ì ì—ì„œëŠ” global minimaëŠ” train minima ì¤‘ì— í•˜ë‚˜ë§Œ ì„ íƒí•˜ë©´ ëœë‹¤

**Implicit regularization effectì—ì„œëŠ” íŒŒë¼ë¯¸í„°ì˜ ì´ˆê¸° ìœ„ì¹˜ì— ë”°ë¼ì„œ test Loss ìƒì—ì„œ ì„ í˜¸ë˜ëŠ” global minimaê°€ ê²°ì •!**

ì´í•´í•˜ê¸° ì‰½ê²Œ **Linear model** ìƒì—ì„œ ê¸°í•˜ì ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆë„ë¡ í•˜ì.

$$ J(\theta) = \frac{1}{n}\sum^n_{i=1}(y^{(i)}-\theta^Tx^{(i)})^2 \quad \{(x^{(1)}, y^{(1)}), \dots ,(x^{(i)}, y^{(i)})\} $$

Over-parameteried ìƒí™©ì„ ê°€ì •í•˜ê¸° ìœ„í•´ì„œ, $n<<d$ ìœ¼ë¡œ ê°€ì •í•œë‹¤.

- $n$ ëŠ” equationì˜ ê°œìˆ˜ì´ê³ , $d$ ëŠ” íŒŒë¼ë¯¸í„°ì˜ ê°œìˆ˜ì´ë‹¤.
- ì´ ê²½ìš°, ìœ„ì—ì„œ ë§í•œ ê²ƒê³¼ ê°™ì´ global minimaê°€ ë§ì•„ì§„ë‹¤.

trainingì„ í†µí•´ $y^{(i)}=\theta^Tx^{(i)}$ ë¥¼ ë§Œì¡±í•˜ëŠ” $\theta$ë¥¼ ì°¾ì•˜ë‹¤ê³  í•˜ì.

- ì´ë•Œ, ì„ í˜•ëŒ€ìˆ˜í•™ì—ì„œ rangeì™€ variable ê·¸ë¦¬ê³  null spaceì˜ ê´€ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ \text{rank}(X) + \text{nullity}(X) = d $$

- ì´ë•Œ, $0=X\theta$ì„ ë§Œì¡±í•˜ëŠ” null spaceì˜ ì°¨ì›ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$ \text{nullity}(X) = d - \text{rank}(X) = d-n $$

**ì´ì œ í•˜ë‚˜ì˜ equationê³¼ 3ê°œì˜ variableë¡œ ê°€ì •í•˜ì.**

- í•´ ê³µê°„ì€ 1ì°¨ì›ì´ ë˜ê³ , ì˜ ê³µê°„ì€ 2ì°¨ì›ì„ì„ ëª…ì‹¬í•˜ì. (ëª¨ë¥´ë©´ ì„ í˜•ëŒ€ìˆ˜í•™ ë³µìŠµ!)

$$ \text{Claim: Gradient Descent with Initial value } \theta=0 \\ \argmin ||\theta||^2_2 \quad \text{s.t. } J(\theta)=0 $$

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/b1908449-8021-4af3-a0dc-bde71d0eeca2/image.png)

ë¶€ë¶„ ê³µê°„ì—ì„œ ì˜ ê³µê°„ì— ê°€ì¥ ê°€ê¹Œìš´ ìµœì†Œ normì„ ê°–ëŠ” ìµœì ì˜ í•´ë¥¼ ì°¾ë„ë¡ í•œë‹¤.

- SGDê°€ $J(\theta)=0$ ì´ ë˜ëŠ” ê°€ì¥ ê°€ê¹Œìš´ ì§€ì ì„ ì°¾ì•„ê°€ê¸° ë•Œë¬¸ì— ì˜³ì€ ê°€ì •ì´ ëœë‹¤.
- $J(\theta)=0$ ì„ ë§Œì¡±í•˜ëŠ” $\theta$ë¥¼ spaní•˜ë„ë¡ í•œë‹¤

$$ \theta \in \text{span}\{\theta^{(1)},\dots,\theta^{(i)}\} $$