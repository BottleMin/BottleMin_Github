
# review...

---

ì§€ë‚œ 3ê°•ì—ì„œ ìš°ë¦¬ëŠ” 3ê°€ì§€ë¥¼ ë°°ì› ë‹¤

![https://velog.velcdn.com/images/bottlemin_park/post/51d21185-1df0-4749-a298-772ed5b6dfa9/image.png](https://velog.velcdn.com/images/bottlemin_park/post/51d21185-1df0-4749-a298-772ed5b6dfa9/image.png)

1. score vector : classifierë¥¼ í†µê³¼í•´ ë‚˜ì˜¨ classì˜ í¬ê¸°
2. Loss function : ë°”ë¡œ ë¶„ë¥˜í•œ ê²°ê³¼ì™€
3. ì‹¤ì œ ê°’ì˜ ì°¨ì´ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” functionì´ë‹¤.
4. regularization : modelì˜ overfittingì„ ë§‰ê¸° ìœ„í•œ ê·œì œ ê¸°ë²•

ì„¸ ê°€ì§€ ê¸°ìˆ ì€ **ìµœì í™” ê¸°ë²•**ì„ í†µí•´ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° $W$ë¥¼ ìµœì í™” ì‹œì¼œì¤„ ìˆ˜ ìˆë‹¤. ê·¸ëŸ´ëŸ¬ë©´ $\nabla_WL$ì„ ì•Œì•„ì•¼ í•œë‹¤.

# Background - ì—°ì‡„ë²•ì¹™ (chain rule)

<aside> ğŸ’¡ í•©ì„± í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ í•©ì„± í•¨ìˆ˜ë¥¼ êµ¬ì„±í•˜ëŠ” ê° í•¨ìˆ˜ì˜ ë¯¸ë¶„ ê³±ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

</aside>

í•©ì„± í•¨ìˆ˜ : ì—¬ëŸ¬ ê°œì˜ í•¨ìˆ˜ë¡œ êµ¬ì„±ëœ í•¨ìˆ˜

**í•¨ìˆ˜ $z=t^2$ì´ê³ ** $t=(x+y)$ì¸ í•©ì„±í•¨ìˆ˜ê°€ ì¡´ì¬í•œë‹¤ê³  í•˜ì. **ë…ë¦½ ë³€ìˆ˜ $x$ì— ëŒ€í•œ ë¯¸ë¶„ì€ $t$ì— ëŒ€í•œ $z$ì˜ ë¯¸ë¶„ê³¼ $x$ì— ëŒ€í•œ $t$ì˜ ë¯¸ë¶„ì˜ ê³±ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.** ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.ã…Œã…Š

$$ \frac{\partial{z}}{\partial{x}}= \frac{\partial{z}}{\partial{t}}\times{\frac{\partial{t}}{\partial{x}}} $$

![Pasted image 20240523143841.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/f61661d1-afb8-4fb8-a3e8-8be859e5a4b0/Pasted_image_20240523143841.png)

# 1. Backpropagation

$$ \begin{array}{|c|c|c|c|} \hline \text{Output Type} & \text{Output Distribution} & \text{Output Layer} & \text{Cost Function} \\ \hline \text{Binary} & \text{Bernoulli} & \text{Sigmoid} & \text{Binary cross-entropy} \\ \hline \text{Discrete} & \text{Multinoulli} & \text{Softmax} & \text{Discrete cross-entropy} \\ \hline \text{Continuous} & \text{Gaussian} & \text{Linear} & \text{Gaussian cross-entropy (MSE)} \\ \hline \text{Continuous} & \text{Mixture of Gaussian} & \text{Mixture Density} & \text{Cross-entropy} \\ \hline \text{Continuous} & \text{Arbitrary} & \text{GAN, VAE, FVBN} & \text{Various} \\ \hline \end{array} $$

ì§€ë‚œ ê°•ì˜ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì—ì„œ gradientë¥¼ **í•´ì„ì **ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ë”ìš± ì •í™•í•˜ê³  ë¹ ë¥¸ ë°©ë²•ì„ì„ ì„¤ëª…ì„ í†µí•´ ë“¤ì—ˆë‹¤.

ê·¸ëŸ¼ ë‹¤ìŒê³¼ ê°™ì€ ì˜ë¬¸ì´ ë“¤ ê²ƒì´ë‹¤.

<aside> ğŸ’¡ Neural Networkì˜ ë¯¸ë¶„ ê³„ì‚°ì„ ì–´ë–»ê²Œ í•´ì•¼í•˜ì§€?? ë¯¸ë¶„ì„ í†µí•´ì„œ ì–´ë–»ê²Œ ê°€ì¤‘ì¹˜ë¥¼ ìµœì í™”ì‹œí‚¨ë‹¤ëŠ” ë§ì¸ê°€??ã…Œ

</aside>

## Computational graph

---

![https://velog.velcdn.com/images/bottlemin_park/post/edc8f1c4-d8d8-4e5b-bbe6-ed79f924aafd/image.png](https://velog.velcdn.com/images/bottlemin_park/post/edc8f1c4-d8d8-4e5b-bbe6-ed79f924aafd/image.png)

fë¼ëŠ” í•¨ìˆ˜(computation)ë¥¼ í•˜ë‚˜ì˜ ë…¸ë“œë¥¼ í‘œí˜„í•œ Computational graphë¥¼ í†µí•´ ë¯¸ë¶„ ê³„ì‚°ê³¼, ê°€ì¤‘ì¹˜ ìµœì í™”ë¥¼ ë³´ê¸° ì‰½ê²Œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

ë…¸ë“œì˜ ì…ë ¥ gradientë¥¼ **Upstream gradient**ë¼ê³  í•˜ê³ , ë…¸ë“œì˜ local gradientì— ì˜í•œ ì¶œë ¥ gradientë¥¼ **Downstream gradient**ë¼ê³  í•œë‹¤.

- Downstream gradientëŠ” **chain rule**ì— ì˜í•´ì„œ Upstream gradient ì •ë³´ë¥¼ ê°€ì§€ê²Œ ëœë‹¤.
- ìœ„ ì‚¬ì§„ì—ì„œ $x$ì— ëŒ€í•œ Downstream gradientëŠ” ì´ë ‡ê²Œ í‘œí˜„í•œë‹¤.

$$ \frac{\partial{L}}{\partial{x}} = \frac{\partial{L}}{\partial{z}}\times\frac{\partial{z}}{\partial{x}} $$

## Patterns in backward flow

---

![https://velog.velcdn.com/images/bottlemin_park/post/3d026460-4c8d-44e9-8304-bafc3b5c86a8/image.png](https://velog.velcdn.com/images/bottlemin_park/post/3d026460-4c8d-44e9-8304-bafc3b5c86a8/image.png)

**Add gate** : ì…ë ¥ì‹ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì—¬ ì£¼ëŠ” ë°©ã…Œì‹ì´ë‹¤. **max gate** : ë¹„êµë˜ëŠ” ë‘ ë³€ìˆ˜ ì¤‘ì—ì„œ maxì— í•´ë‹¹ëœ ë³€ìˆ˜ì— gradient ê°’ì„ í†µê³¼ì‹œì¼œì¤€ë‹¤. **mul gate** : ë‘ ë³€ìˆ˜ê°’ì„ switchã…Œí•´ì¤€ë‹¤.

## Scalar operation

---

### example 1

$$ f(x,y,z)=(x+y)z $$

![https://velog.velcdn.com/images/bottlemin_park/post/8d7d5063-ecbf-4794-94ed-45a1ba9d6123/image.png](https://velog.velcdn.com/images/bottlemin_park/post/8d7d5063-ecbf-4794-94ed-45a1ba9d6123/image.png)

$x=-2, y=5, z=-4$ë¼ê³  ì˜ˆì‹œê°€ ì£¼ì–´ì¡Œë‹¤ê³  í•˜ì

í•¨ìˆ˜ $f$ëŠ” ìœ„ì˜ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

ì´ì œë¶€í„° ë³€ìˆ˜ $x,y,z$ ì— ëŒ€í•œ gradientë¥¼ ê³„ì‚°í•´ì•¼í•œë‹¤. ê°ê°ì˜ ë…¸ë“œë¥¼ í‘œí˜„í•˜ê²Œ ëœë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ìŒ í•¨ìˆ˜ë¡œ ì´ë¤„ì§„ë‹¤.

$$ f=qz \\ q = x+y $$

ê°ê°ì˜ backpropagationì„ êµ¬í•˜ê¸° ìœ„í•´ chain ruleë¥¼ ì ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤.

### example 2

$$ f(w,x)=\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2x_2)}} $$

ì— ëŒ€í•œ backpropagationì€? ë¨¼ì € computational graphë¡œ í‘œí˜„í•´ë³´ì

![https://velog.velcdn.com/images/bottlemin_park/post/af131c66-c390-457f-9d8a-3f66484f5c1a/image.png](https://velog.velcdn.com/images/bottlemin_park/post/af131c66-c390-457f-9d8a-3f66484f5c1a/image.png)

<aside> ğŸ’¡ $\sigma(x)=\frac{1}{1+e^{-x}}$ëŠ” sigmoid functionìœ¼ë¡œì¨ classificationì—ì„œ ì“°ì´ëŠ” activation function ì¤‘ì— í•˜ë‚˜ì´ë‹¤.

</aside>

- ì—¬ê¸°ì„œ sigmoid functionì— ëŒ€í•œ ë¯¸ë¶„ ê³„ì‚°ì„ ì‚¬ì „ì— ì •ì˜í•´ì¤„ ìˆ˜ ìˆë‹¤ë©´ ê³„ì‚° ë¹„ìš©ë„ ì•„ë‚„ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?

$$ \frac{d\sigma(x)}{dx}=\frac{e^{-x}}{1+e^{-x}}=\Big(\frac{1+e^{-x}-1}{1+e^{-x}}\Big) \Big(\frac{1}{1+e^{-x}}\Big)=(1-\sigma(x))(\sigma(x))

$$

ì´ë ‡ê²Œ sigmoid functionì„ í•˜ë‚˜ì˜ big nodeë¡œ ë³¼ ìˆ˜ ìˆë‹¤.

## Vectorized operations (Affine ê³„ì¸µ)

---

![https://velog.velcdn.com/images/bottlemin_park/post/1d5798c7-7973-412d-b8ce-1b8ab04951be/image.png](https://velog.velcdn.com/images/bottlemin_park/post/1d5798c7-7973-412d-b8ce-1b8ab04951be/image.png)

ëª¨ë“  Neural Networkì˜ ê²½ìš°ì—ëŠ” ë²¡í„°(í–‰ë ¬) í˜•ì‹ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì´ ì§„í–‰ëœë‹¤.

ê°„ë‹¨í•˜ê²Œ ê³„ì‚° ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ìë©´

![Pasted image 20240523144625.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/6a6fb1fd-94c3-49e6-a01a-dbe4955dd303/Pasted_image_20240523144625.png)

ê³„ì‚° ê·¸ë˜í”„ì˜ ê° ë…¸ë“œëŠ” í–‰ë ¬ë¡œ ì´ë¤„ì ¸ ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ê²½ìš°ì—ì„œëŠ” ì–´ë–»ê²Œ backpropagationì´ ì§„í–‰ë  ê²ƒì¸ê°€?

$$ \begin{aligned} \frac{df}{dx_1} =& [\frac{df}{dx_1},\; 0, \; \dots, \; 0] \\ \frac{df}{dx_2} =& [0, \; \frac{df}{dx_2}, \; \dots, \; 0] \\ &\vdots \\ \frac{df}{dx_{4096}} =& [0, \; \dots, \; \frac{df}{dx_{4096}}] \end{aligned}

$$

ã…Œ $4096\times4096$ í¬ê¸°ì˜ Jacobian í–‰ë ¬ ê³„ì‚°ì„ í•´ì¤˜ì•¼í•œë‹¤.

ì‚¬ì‹¤ ì´ ë¿ë§Œ ì•„ë‹ˆë¼ mini batch sizeê°€ 100ê°œë¼ê³  í•œë‹¤ë©´, input vectorëŠ” $100\times4096$ì´ ë  ê²ƒì´ê³ , Jacobian í–‰ë ¬ì˜ í¬ê¸°ëŠ” ë¬´ë ¤ $[409,600\times409,600]$ê°€ ë  ê²ƒì´ë‹¤.

**ê·¸ëŸ¬ë‚˜ í–‰ë ¬ì‹ì„ ì˜ ë³´ë©´ Jacobian í–‰ë ¬ì€ ëŒ€ê°í–‰ë ¬ì´ë¯€ë¡œ êµ³ì´ í–‰ë ¬ ì „ì²´ë¥¼ ê³„ì‚°í•´ì¤„ í•„ìš”ê°€ ì—†ê³ , ì¶œë ¥ì— í•´ë‹¹ëœ ìš”ì†Œì—ë§Œ backpropagationì„ ì§„í–‰í•´ì£¼ë©´ ëœë‹¤.**

![https://velog.velcdn.com/images/bottlemin_park/post/ea57c002-31b0-4745-8106-c73d31d39e8a/image.png](https://velog.velcdn.com/images/bottlemin_park/post/ea57c002-31b0-4745-8106-c73d31d39e8a/image.png)

$$

\begin{aligned} q=W\cdot{x}=\begin{pmatrix} W_{1,1}x_1+ &\cdots &+W_{1,n}x_n \\ & \vdots \\ W_{n,1}x_1+ &\cdots &+W_{n,n}x_n \end{pmatrix} \space\space <Vector> \end{aligned} \\ f(q)=\lVert{q}\rVert^2=q^2_1+\cdots+q^2_n \space\space <scalar>

$$

ìš”ì†Œ ë³„($W, x$) gradientëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤.

$$ \begin{aligned} \frac{\partial{L}}{\partial{x}}&=\frac{\partial{L}}{\partial{q}}\times \frac{\partial{q}}{\partial{x}} \\ \frac{\partial{L}}{\partial{W}}&=\frac{\partial{L}}{\partial{q}}\times \frac{\partial{q}}{\partial{W}} \end{aligned} $$

$\frac{\partial{L}}{\partial{q}}$ì€ gradientë¥¼ êµ¬í•˜ë©´ ë˜ëŠ”ë°, **$\frac{\partial{q}}{\partial{x}} \& \frac{\partial{q}}{\partial{W}}$ëŠ” ì–´ë–»ê²Œ ê³„ì‚°í•´ì•¼í• ê¹Œ?**

**1. $\frac{\partial{q}}{\partial{x}}$ ì— ëŒ€í•œ ë¯¸ë¶„**

$$ \begin{aligned} \frac{\partial{q}}{\partial{x}}&=\begin{aligned}\begin{pmatrix} \frac{\partial{q}}{\partial{x_1}}\\ \frac{\partial{q}}{\partial{x_2}}\\ \vdots\\ \frac{\partial{q}}{\partial{x_n}}\end{pmatrix}\end{aligned}=\begin{pmatrix} W_{1,1} & W_{2,1} & \cdots & W_{n,1} \\ W_{1,2} & W_{2,2} & \cdots & W_{n,2} \\ & & \vdots & \\ W_{1,n} & W_{2,n} & \cdots &W_{n,n} \end{pmatrix} \\&= W^T \end{aligned} $$

**2. $\frac{\partial{q}}{\partial{W}}$ ì— ëŒ€í•œ ë¯¸ë¶„**

$$ \begin{aligned} \frac{\partial{q}}{\partial{W}} &= \begin{aligned} \begin{pmatrix} \frac{\partial{q}}{\partial{W_{1,1}}} & \frac{\partial{q}}{\partial{W_{1,2}}} & \cdots & \frac{\partial{q}}{\partial{W_{1,n}}}\\ \frac{\partial{q}}{\partial{W_{2,1}}} & \frac{\partial{q}}{\partial{W_{2,2}}} & \cdots & \frac{\partial{q}}{\partial{W_{2,n}}}\\ & & \vdots\\ \frac{\partial{q}}{\partial{W_{n,1}}} & \frac{\partial{q}}{\partial{W_{n,2}}} & \cdots & \frac{\partial{q}}{\partial{W_{n,n}}}\\ \end{pmatrix} \end{aligned} \\ &= \begin{aligned} \begin{pmatrix} x_1 & x_2 & \cdots & x_n \\ x_1 & x_2 & \cdots & x_n \\ & & \vdots \\ x_1 & x_2 & \cdots & x_n \\ \end{pmatrix} \end{aligned} = (x_1, x_2, \cdots, x_n ) = x^T \end{aligned} $$

ã…Œí–‰ë ¬ ë¯¸ë¶„ì˜ ë²•ì¹™ì„ ì•Œê¸°ë§Œ í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤

$$ \begin{aligned} \frac{\partial{L}}{\partial{x}}&=\frac{\partial{L}}{\partial{q}}\times \frac{\partial{q}}{\partial{x}} = \frac{\partial{L}}{\partial{q}}\space{W^T}=2W^T\cdot q \\ \frac{\partial{L}}{\partial{W}}&=\frac{\partial{L}}{\partial{q}}\times \frac{\partial{q}}{\partial{W}} = x^T\space{\frac{\partial{L}}{\partial{q}}}=2q \cdot x^T \end{aligned} $$

![Pasted image 20240523145239.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/ee31428e-f63a-455f-bcf0-5a9de949cc86/5a3204ab-16c8-46fc-8dd6-d0e793b5dbf4/Pasted_image_20240523145239.png)